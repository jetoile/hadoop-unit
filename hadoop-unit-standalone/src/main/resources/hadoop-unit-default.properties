#HADOOP_HOME=/opt/hadoop

tmp.dir.path=/tmp

zookeeper.artifactId=fr.jetoile.hadoop:hadoop-unit-zookeeper:${project.version}
zookeeper.mainClass=fr.jetoile.hadoopunit.component.ZookeeperBootstrap
zookeeper.metadataClass=fr.jetoile.hadoopunit.component.ZookeeperMetadata

hdfs.artifactId=fr.jetoile.hadoop:hadoop-unit-hdfs:${project.version}
hdfs.mainClass=fr.jetoile.hadoopunit.component.HdfsBootstrap
hdfs.metadataClass=fr.jetoile.hadoopunit.component.HdfsMetadata

hdfs3.artifactId=fr.jetoile.hadoop:hadoop-unit-hdfs3:${project.version}
hdfs3.mainClass=fr.jetoile.hadoopunit.component.Hdfs3Bootstrap
hdfs3.metadataClass=fr.jetoile.hadoopunit.component.Hdfs3Metadata

alluxio.artifactId=fr.jetoile.hadoop:hadoop-unit-alluxio:${project.version}
alluxio.mainClass=fr.jetoile.hadoopunit.component.AlluxioBootstrap
alluxio.metadataClass=fr.jetoile.hadoopunit.component.AlluxioMetadata

hivemeta.artifactId=fr.jetoile.hadoop:hadoop-unit-hive:${project.version}
hivemeta.mainClass=fr.jetoile.hadoopunit.component.HiveMetastoreBootstrap
hivemeta.metadataClass=fr.jetoile.hadoopunit.component.HiveMetastoreMetadata

hiveserver2.artifactId=fr.jetoile.hadoop:hadoop-unit-hive:${project.version}
hiveserver2.mainClass=fr.jetoile.hadoopunit.component.HiveServer2Bootstrap
hiveserver2.metadataClass=fr.jetoile.hadoopunit.component.HiveServer2Metadata

hivemeta3.artifactId=fr.jetoile.hadoop:hadoop-unit-hivemeta3:${project.version}
hivemeta3.mainClass=fr.jetoile.hadoopunit.component.HiveMetastore3Bootstrap
hivemeta3.metadataClass=fr.jetoile.hadoopunit.component.HiveMetastore3Metadata

hiveserver23.artifactId=fr.jetoile.hadoop:hadoop-unit-hiveserver23:${project.version}
hiveserver23.mainClass=fr.jetoile.hadoopunit.component.HiveServer23Bootstrap
hiveserver23.metadataClass=fr.jetoile.hadoopunit.component.HiveServer23Metadata

kafka.artifactId=fr.jetoile.hadoop:hadoop-unit-kafka:${project.version}
kafka.mainClass=fr.jetoile.hadoopunit.component.KafkaBootstrap
kafka.metadataClass=fr.jetoile.hadoopunit.component.KafkaMetadata

hbase.artifactId=fr.jetoile.hadoop:hadoop-unit-hbase:${project.version}
hbase.mainClass=fr.jetoile.hadoopunit.component.HBaseBootstrap
hbase.metadataClass=fr.jetoile.hadoopunit.component.HBaseMetadata

oozie.artifactId=fr.jetoile.hadoop:hadoop-unit-oozie:${project.version}
oozie.mainClass=fr.jetoile.hadoopunit.component.OozieBootstrap
oozie.metadataClass=fr.jetoile.hadoopunit.component.OozieMetadata

solr.artifactId=fr.jetoile.hadoop:hadoop-unit-solr:${project.version}
solr.mainClass=fr.jetoile.hadoopunit.component.SolrBootstrap
solr.metadataClass=fr.jetoile.hadoopunit.component.SolrMetadata

solrcloud.artifactId=fr.jetoile.hadoop:hadoop-unit-solrcloud:${project.version}
solrcloud.mainClass=fr.jetoile.hadoopunit.component.SolrCloudBootstrap
solrcloud.metadataClass=fr.jetoile.hadoopunit.component.SolrCloudMetadata

cassandra.artifactId=fr.jetoile.hadoop:hadoop-unit-cassandra:${project.version}
cassandra.mainClass=fr.jetoile.hadoopunit.component.CassandraBootstrap
cassandra.metadataClass=fr.jetoile.hadoopunit.component.CassandraMetadata

mongodb.artifactId=fr.jetoile.hadoop:hadoop-unit-mongodb:${project.version}
mongodb.mainClass=fr.jetoile.hadoopunit.component.MongoDbBootstrap
mongodb.metadataClass=fr.jetoile.hadoopunit.component.MongoDbMetadata

elasticsearch.artifactId=fr.jetoile.hadoop:hadoop-unit-elasticsearch:${project.version}
elasticsearch.mainClass=fr.jetoile.hadoopunit.component.ElasticSearchBootstrap
elasticsearch.metadataClass=fr.jetoile.hadoopunit.component.ElasticSearchMetadata

neo4j.artifactId=fr.jetoile.hadoop:hadoop-unit-neo4j:${project.version}
neo4j.mainClass=fr.jetoile.hadoopunit.component.Neo4jBootstrap
neo4j.metadataClass=fr.jetoile.hadoopunit.component.Neo4jMetadata

knox.artifactId=fr.jetoile.hadoop:hadoop-unit-knox:${project.version}
knox.mainClass=fr.jetoile.hadoopunit.component.KnoxBootstrap
knox.metadataClass=fr.jetoile.hadoopunit.component.KnoxMetadata

redis.artifactId=fr.jetoile.hadoop:hadoop-unit-redis:${project.version}
redis.mainClass=fr.jetoile.hadoopunit.component.RedisBootstrap
redis.metadataClass=fr.jetoile.hadoopunit.component.RedisMetadata

yarn.artifactId=fr.jetoile.hadoop:hadoop-unit-yarn:${project.version}
yarn.mainClass=fr.jetoile.hadoopunit.component.YarnBootstrap
yarn.metadataClass=fr.jetoile.hadoopunit.component.YarnMetadata

yarn3.artifactId=fr.jetoile.hadoop:hadoop-unit-yarn3:${project.version}
yarn3.mainClass=fr.jetoile.hadoopunit.component.Yarn3Bootstrap
yarn3.metadataClass=fr.jetoile.hadoopunit.component.Yarn3Metadata

confluent_kafka.artifactId=fr.jetoile.hadoop:hadoop-unit-confluent:${project.version}
confluent_kafka.mainClass=fr.jetoile.hadoopunit.component.ConfluentKafkaBootstrap
confluent_kafka.metadataClass=fr.jetoile.hadoopunit.component.ConfluentKafkaMetadata

confluent_schemaregistry.artifactId=fr.jetoile.hadoop:hadoop-unit-confluent:${project.version}
confluent_schemaregistry.mainClass=fr.jetoile.hadoopunit.component.ConfluentSchemaRegistryBootstrap
confluent_schemaregistry.metadataClass=fr.jetoile.hadoopunit.component.ConfluentSchemaRegistryMetadata

confluent_ksql_rest.artifactId=fr.jetoile.hadoop:hadoop-unit-confluent:${project.version}
confluent_ksql_rest.mainClass=fr.jetoile.hadoopunit.component.ConfluentKsqlRestBootstrap
confluent_ksql_rest.metadataClass=fr.jetoile.hadoopunit.component.ConfluentKsqlRestMetadata

confluent_kafka_rest.artifactId=fr.jetoile.hadoop:hadoop-unit-confluent-rest:${project.version}
confluent_kafka_rest.mainClass=fr.jetoile.hadoopunit.component.ConfluentKafkaRestBootstrap
confluent_kafka_rest.metadataClass=fr.jetoile.hadoopunit.component.ConfluentKafkaRestMetadata

docker.artifactId=fr.jetoile.hadoop:hadoop-unit-docker:${project.version}
docker.mainClass=fr.jetoile.hadoopunit.component.DockerBootstrap
docker.metadataClass=fr.jetoile.hadoopunit.component.DockerMetadata

docker_compose.artifactId=fr.jetoile.hadoop:hadoop-unit-dockercompose:${project.version}
docker_compose.mainClass=fr.jetoile.hadoopunit.component.DockerComposeBootstrap
docker_compose.metadataClass=fr.jetoile.hadoopunit.component.DockerComposeMetadata

maven.central.repo=https://repo.maven.apache.org/maven2/
maven.local.repo=~/.m2/repository

maven.debug=true

# Zookeeper
zookeeper.temp.dir=/embedded_zk
zookeeper.host=127.0.0.1
zookeeper.port=22010

# Hive
hive.scratch.dir=/hive_scratch_dir
hive.warehouse.dir=/tmp/warehouse_dir

# Hive Metastore
hive.metastore.hostname=127.0.0.1
hive.metastore.port=20102
hive.metastore.derby.db.dir=/metastore_db

# Hive Server2
hive.server2.hostname=127.0.0.1
hive.server2.port=20103

# Hive Test
hive.test.database.name=default
hive.test.table.name=test_table


# Hive3
hive3.scratch.dir=/tmp/hive_scratch_dir
hive3.warehouse.dir=/tmp/warehouse_dir

# Hive Metastore 3
hive3.metastore.hostname=localhost
hive3.metastore.port=20102
hive3.metastore.derby.db.dir=metastore_db

# Hive Server2 3
hive3.server2.hostname=localhost
hive3.server2.port=20103

# Hive Test 3
hive3.test.database.name=default
hive3.test.table.name=test_table


# HDFS
hdfs.namenode.host=127.0.0.1
hdfs.namenode.port=20112
hdfs.namenode.http.port=50070
hdfs.temp.dir=/embedded_hdfs
hdfs.num.datanodes=1
hdfs.enable.permissions=false
hdfs.format=true
hdfs.enable.running.user.as.proxy.user=true
hdfs.datanode.address=127.0.0.1:50010
hdfs.datanode.http.address=127.0.0.1:50075
hdfs.datanode.ipc.address=127.0.0.1:50020

# HDFS Test
hdfs.test.file=/tmp/testing
hdfs.test.string=TESTING


# HDFS3
hdfs3.namenode.host=127.0.0.1
hdfs3.namenode.port=20112
hdfs3.namenode.http.port=50070
hdfs3.temp.dir=/tmp/embedded_hdfs
hdfs3.num.datanodes=1
hdfs3.enable.permissions=false
hdfs3.format=true
hdfs3.enable.running.user.as.proxy.user=true
hdfs3.datanode.address=127.0.0.1:50010
hdfs3.datanode.http.address=127.0.0.1:50075
hdfs3.datanode.ipc.address=127.0.0.1:50020

# HDFS3 Test
hdfs3.test.file=/tmp/testing
hdfs3.test.string=TESTING



# HBase
hbase.master.port=25111
hbase.master.info.port=-1
hbase.num.region.servers=1
hbase.root.dir=/embedded_hbase
hbase.znode.parent=/hbase-unsecure
hbase.wal.replication.enabled=false

# HBase REST
hbase.rest.port=28000
hbase.rest.readonly=false
hbase.rest.info.port=28080
hbase.rest.host=0.0.0.0
hbase.rest.threads.max=100
hbase.rest.threads.min=2

# HBase Test
hbase.test.table.name=hbase_test_table
hbase.test.col.family.name=cf1
hbase.test.col.qualifier.name=cq1
hbase.test.num.rows.to.put=50

# Kafka
kafka.hostname=127.0.0.1
kafka.port=20111

# Kafka Test
kafka.test.topic=testtopic
kafka.test.message.count=10
kafka.test.broker.id=1
kafka.test.temp.dir=/embedded_kafka

#SolR + SolRCloud
solr.dir=solr

#SolR
solr.collection.internal.name=collection1_shard1_replica1

#SolRCloud
solr.collection.name=collection1
solr.cloud.port=8983





# YARN
yarn.num.node.managers=1
yarn.num.local.dirs=1
yarn.num.log.dirs=1
yarn.resource.manager.address=127.0.0.1:37001
yarn.resource.manager.hostname=127.0.0.1
yarn.resource.manager.scheduler.address=127.0.0.1:37002
yarn.resource.manager.resource.tracker.address=127.0.0.1:37003
yarn.resource.manager.webapp.address=127.0.0.1:37004
yarn.use.in.jvm.container.executor=false


# YARN3
yarn3.num.node.managers=1
yarn3.num.local.dirs=1
yarn3.num.log.dirs=1
yarn3.resource.manager.address=localhost:37001
yarn3.resource.manager.hostname=localhost
yarn3.resource.manager.scheduler.address=localhost:37002
yarn3.resource.manager.resource.tracker.address=localhost:37003
yarn3.resource.manager.webapp.address=localhost:37004
yarn3.use.in.jvm.container.executor=false


# MR
mr.job.history.address=127.0.0.1:37005

# Oozie
oozie.tmp.dir=/oozie_tmp
oozie.test.dir=/embedded_oozie
oozie.home.dir=/oozie_home
oozie.username=blah
oozie.groupname=testgroup
oozie.hdfs.share.lib.dir=/tmp/share_lib
oozie.share.lib.create=true
oozie.local.share.lib.cache.dir=/tmp/share_lib_cache
oozie.purge.local.share.lib.cache=false
oozie.sharelib.path=~/github
oozie.sharelib.name=oozie-4.2.0.2.6.5.0-292-distro.tar.gz
oozie.port=20113
oozie.host=127.0.0.1
oozie.sharelib.component=OOZIE,MAPREDUCE_STREAMING,SPARK
#oozie.sharelib.component=OOZIE,HCATALOG,DISTCP,MAPREDUCE_STREAMING,PIG,HIVE,HIVE2,SQOOP,SPARK

# ElasticSearch
elasticsearch.version=6.7.1
elasticsearch.ip=127.0.0.1
elasticsearch.http.port=14433
elasticsearch.tcp.port=14533
elasticsearch.index.name=test_index
elasticsearch.cluster.name=elasticsearch
#elasticsearch.download.url=https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.4.3.zip

# MongoDB
mongo.ip=127.0.0.1
mongo.port=13333
mongo.database.name=test_database
mongo.collection.name=test_collection

# Cassandra
cassandra.ip=127.0.0.1
cassandra.port=13433
cassandra.temp.dir=/embedded_cassandra

# Neo4j
neo4j.ip=127.0.0.1
neo4j.port=13533
neo4j.temp.dir=/embedded_neo4j

# KNOX
knox.host=127.0.0.1
knox.port=8888
knox.path=gateway
knox.cluster=mycluster
knox.home.dir=/embedded_knox
knox.service=namenode,webhdfs,webhbase
#knox.service=namenode,webhdfs,webhbase,oozie

# Alluxio
#alluxio.work.dir=/tmp/alluxio
alluxio.work.dir=hdfs://127.0.0.1:20112/alluxio
alluxio.hostname=127.0.0.1
alluxio.master.port=14001
alluxio.master.web.port=14002
alluxio.proxy.web.port=14100
alluxio.worker.web.port=14003
alluxio.worker.data.port=14004
alluxio.worker.port=14005
alluxio.webapp.directory=conf/alluxio/webapp


# Redis
redis.port=6379
redis.download.url=http://download.redis.io/releases/
redis.version=5.0.4
redis.cleanup.installation=false
redis.temp.dir=/redis
redis.type=SERVER
#redis.type=CLUSTER
#redis.type=MASTER_SLAVE
#redis.type=SENTINEL
#redis.slave.ports=6380
#redis.sentinel.ports=36479,36480,36481,36482,36483




# Confluent
confluent.schemaregistry.port=8081
confluent.schemaregistry.host=127.0.0.1
confluent.schemaregistry.kafkastore.topic=_schema
confluent.schemaregistry.debug=false

confluent.kafka.log.dirs=/kafka-logs
confluent.kafka.broker.id=0
confluent.kafka.port=22222
confluent.kafka.host=127.0.0.1

confluent.rest.host=127.0.0.1
confluent.rest.port=8082

confluent.ksql.host=127.0.0.1
confluent.ksql.port=8083



# Docker
docker.imagename=alpine:3.2
docker.exposedports=80
docker.envs=MAGIC_NUMBER:42
docker.labels=MAGIC_NUMBER:42
docker.command=/bin/sh, -c, while true; do echo \"$MAGIC_NUMBER\" | nc -l -p 80; done
docker.fixed.exposedports=21300:80
#docker.classpath.resources.mapping=hadoop-unit-default.properties:/hadoop-unit-default.properties:READ_ONLY

# Docker compose
dockercompose.filename=conf/docker-compose.yml
#dockercompose.exposedports=zoo:2181,resourcemanager:8088
dockercompose.local=false