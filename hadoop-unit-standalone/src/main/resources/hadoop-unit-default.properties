#HADOOP_HOME=/opt/hadoop

zookeeper.artifactId=fr.jetoile.hadoop:hadoop-unit-zookeeper:${project.version}
zookeeper.mainClass=fr.jetoile.hadoopunit.component.ZookeeperBootstrap
zookeeper.metadataClass=fr.jetoile.hadoopunit.component.ZookeeperMetadata

hdfs.artifactId=fr.jetoile.hadoop:hadoop-unit-hdfs:${project.version}
hdfs.mainClass=fr.jetoile.hadoopunit.component.HdfsBootstrap
hdfs.metadataClass=fr.jetoile.hadoopunit.component.HdfsMetadata

alluxio.artifactId=fr.jetoile.hadoop:hadoop-unit-alluxio:${project.version}
alluxio.mainClass=fr.jetoile.hadoopunit.component.AlluxioBootstrap
alluxio.metadataClass=fr.jetoile.hadoopunit.component.AlluxioMetadata

hivemeta.artifactId=fr.jetoile.hadoop:hadoop-unit-hive:${project.version}
hivemeta.mainClass=fr.jetoile.hadoopunit.component.HiveMetastoreBootstrap
hivemeta.metadataClass=fr.jetoile.hadoopunit.component.HiveMetastoreMetadata

hiveserver2.artifactId=fr.jetoile.hadoop:hadoop-unit-hive:${project.version}
hiveserver2.mainClass=fr.jetoile.hadoopunit.component.HiveServer2Bootstrap
hiveserver2.metadataClass=fr.jetoile.hadoopunit.component.HiveServer2Metadata

hivemeta_2.artifactId=fr.jetoile.hadoop:hadoop-unit-hive2:${project.version}
hivemeta_2.mainClass=
hivemeta_2.metadataClass=

hiveserver2_2.artifactId=fr.jetoile.hadoop:hadoop-unit-hive2:${project.version}
hiveserver2_2.mainClass=
hiveserver2_2.metadataClass=

hivemeta_3.artifactId=fr.jetoile.hadoop:hadoop-unit-hive3:${project.version}
hivemeta_3.mainClass=
hivemeta_3.metadataClass=

hiveserver2_3.artifactId=fr.jetoile.hadoop:hadoop-unit-hive3:${project.version}
hiveserver2_3.mainClass=
hiveserver2_3.metadataClass=

kafka.artifactId=fr.jetoile.hadoop:hadoop-unit-kafka:${project.version}
kafka.mainClass=fr.jetoile.hadoopunit.component.KafkaBootstrap
kafka.metadataClass=fr.jetoile.hadoopunit.component.KafkaMetadata

hbase.artifactId=fr.jetoile.hadoop:hadoop-unit-hbase:${project.version}
hbase.mainClass=fr.jetoile.hadoopunit.component.HBaseBootstrap
hbase.metadataClass=fr.jetoile.hadoopunit.component.HBaseMetadata

oozie.artifactId=fr.jetoile.hadoop:hadoop-unit-oozie:${project.version}
oozie.mainClass=fr.jetoile.hadoopunit.component.OozieBootstrap
oozie.metadataClass=fr.jetoile.hadoopunit.component.OozieMetadata

solr.artifactId=fr.jetoile.hadoop:hadoop-unit-solr:${project.version}
solr.mainClass=fr.jetoile.hadoopunit.component.SolrBootstrap
solr.metadataClass=fr.jetoile.hadoopunit.component.SolrMetadata

solrcloud.artifactId=fr.jetoile.hadoop:hadoop-unit-solrcloud:${project.version}
solrcloud.mainClass=fr.jetoile.hadoopunit.component.SolrCloudBootstrap
solrcloud.metadataClass=fr.jetoile.hadoopunit.component.SolrCloudMetadata

cassandra.artifactId=fr.jetoile.hadoop:hadoop-unit-cassandra:${project.version}
cassandra.mainClass=fr.jetoile.hadoopunit.component.CassandraBootstrap
cassandra.metadataClass=fr.jetoile.hadoopunit.component.CassandraMetadata

mongodb.artifactId=fr.jetoile.hadoop:hadoop-unit-mongodb:${project.version}
mongodb.mainClass=fr.jetoile.hadoopunit.component.MongoDbBootstrap
mongodb.metadataClass=fr.jetoile.hadoopunit.component.MongoDbMetadata

elasticsearch.artifactId=fr.jetoile.hadoop:hadoop-unit-elasticsearch:${project.version}
elasticsearch.mainClass=fr.jetoile.hadoopunit.component.ElasticSearchBootstrap
elasticsearch.metadataClass=fr.jetoile.hadoopunit.component.ElasticSearchMetadata

neo4j.artifactId=fr.jetoile.hadoop:hadoop-unit-neo4j:${project.version}
neo4j.mainClass=fr.jetoile.hadoopunit.component.Neo4jBootstrap
neo4j.metadataClass=fr.jetoile.hadoopunit.component.Neo4jMetadata

knox.artifactId=fr.jetoile.hadoop:hadoop-unit-knox:${project.version}
knox.mainClass=fr.jetoile.hadoopunit.component.KnoxBootstrap
knox.metadataClass=fr.jetoile.hadoopunit.component.KnoxMetadata

redis.artifactId=fr.jetoile.hadoop:hadoop-unit-redis:${project.version}
redis.mainClass=fr.jetoile.hadoopunit.component.RedisBootstrap
redis.metadataClass=fr.jetoile.hadoopunit.component.RedisMetadata

yarn.artifactId=fr.jetoile.hadoop:hadoop-unit-yarn:${project.version}
yarn.mainClass=fr.jetoile.hadoopunit.component.YarnBootstrap
yarn.metadataClass=fr.jetoile.hadoopunit.component.YarnMetadata

confluent_kafka.artifactId=fr.jetoile.hadoop:hadoop-unit-confluent:${project.version}
confluent_kafka.mainClass=fr.jetoile.hadoopunit.component.ConfluentKafkaBootstrap
confluent_kafka.metadataClass=fr.jetoile.hadoopunit.component.ConfluentKafkaMetadata

confluent_schemaregistry.artifactId=fr.jetoile.hadoop:hadoop-unit-confluent:${project.version}
confluent_schemaregistry.mainClass=fr.jetoile.hadoopunit.component.ConfluentSchemaRegistryBootstrap
confluent_schemaregistry.metadataClass=fr.jetoile.hadoopunit.component.ConfluentSchemaRegistryMetadata

confluent_ksql_rest.artifactId=fr.jetoile.hadoop:hadoop-unit-confluent:${project.version}
confluent_ksql_rest.mainClass=fr.jetoile.hadoopunit.component.ConfluentKsqlRestBootstrap
confluent_ksql_rest.metadataClass=fr.jetoile.hadoopunit.component.ConfluentKsqlRestMetadata

confluent_kafka_rest.artifactId=fr.jetoile.hadoop:hadoop-unit-confluent-rest:${project.version}
confluent_kafka_rest.mainClass=fr.jetoile.hadoopunit.component.ConfluentKafkaRestBootstrap
confluent_kafka_rest.metadataClass=fr.jetoile.hadoopunit.component.ConfluentKafkaRestMetadata

docker.artifactId=fr.jetoile.hadoop:hadoop-unit-docker:${project.version}
docker.mainClass=fr.jetoile.hadoopunit.component.DockerBootstrap
docker.metadataClass=fr.jetoile.hadoopunit.component.DockerMetadata

docker_compose.artifactId=fr.jetoile.hadoop:hadoop-unit-dockercompose:${project.version}
docker_compose.mainClass=fr.jetoile.hadoopunit.component.DockerComposeBootstrap
docker_compose.metadataClass=fr.jetoile.hadoopunit.component.DockerComposeMetadata

maven.central.repo=https://repo.maven.apache.org/maven2/
maven.local.repo=~/.m2/repository

maven.debug=true

# Zookeeper
zookeeper.temp.dir=/tmp/embedded_zk
zookeeper.host=127.0.0.1
zookeeper.port=22010

# Hive
hive.scratch.dir=/tmp/hive_scratch_dir
hive.warehouse.dir=/tmp/warehouse_dir

# Hive Metastore
hive.metastore.hostname=127.0.0.1
hive.metastore.port=20102
hive.metastore.derby.db.dir=metastore_db

# Hive Server2
hive.server2.hostname=127.0.0.1
hive.server2.port=20103

# Hive Test
hive.test.database.name=default
hive.test.table.name=test_table


# HDFS
hdfs.namenode.host=127.0.0.1
hdfs.namenode.port=20112
hdfs.namenode.http.port=50070
hdfs.temp.dir=/tmp/embedded_hdfs
hdfs.num.datanodes=1
hdfs.enable.permissions=false
hdfs.format=true
hdfs.enable.running.user.as.proxy.user=true
hdfs.datanode.address=127.0.0.1:50010
hdfs.datanode.http.address=127.0.0.1:50075
hdfs.datanode.ipc.address=127.0.0.1:50020

# HDFS Test
hdfs.test.file=/tmp/testing
hdfs.test.string=TESTING


# HBase
hbase.master.port=25111
hbase.master.info.port=-1
hbase.num.region.servers=1
hbase.root.dir=/tmp/embedded_hbase
hbase.znode.parent=/hbase-unsecure
hbase.wal.replication.enabled=false

# HBase REST
hbase.rest.port=28000
hbase.rest.readonly=false
hbase.rest.info.port=28080
hbase.rest.host=0.0.0.0
hbase.rest.threads.max=100
hbase.rest.threads.min=2

# HBase Test
hbase.test.table.name=hbase_test_table
hbase.test.col.family.name=cf1
hbase.test.col.qualifier.name=cq1
hbase.test.num.rows.to.put=50

# Kafka
kafka.hostname=127.0.0.1
kafka.port=20111

# Kafka Test
kafka.test.topic=testtopic
kafka.test.message.count=10
kafka.test.broker.id=1
kafka.test.temp.dir=embedded_kafka

#SolR + SolRCloud
solr.dir=solr

#SolR
solr.collection.internal.name=collection1_shard1_replica1

#SolRCloud
solr.collection.name=collection1
solr.cloud.port=8983





# YARN
yarn.num.node.managers=1
yarn.num.local.dirs=1
yarn.num.log.dirs=1
yarn.resource.manager.address=127.0.0.1:37001
yarn.resource.manager.hostname=127.0.0.1
yarn.resource.manager.scheduler.address=127.0.0.1:37002
yarn.resource.manager.resource.tracker.address=127.0.0.1:37003
yarn.resource.manager.webapp.address=127.0.0.1:37004
yarn.use.in.jvm.container.executor=false

# MR
mr.job.history.address=127.0.0.1:37005

# Oozie
oozie.tmp.dir=/tmp/oozie_tmp
oozie.test.dir=/tmp/embedded_oozie
oozie.home.dir=/tmp/oozie_home
oozie.username=blah
oozie.groupname=testgroup
oozie.hdfs.share.lib.dir=/tmp/share_lib
oozie.share.lib.create=true
oozie.local.share.lib.cache.dir=/tmp/share_lib_cache
oozie.purge.local.share.lib.cache=false
oozie.sharelib.path=~/github
oozie.sharelib.name=oozie-4.2.0.2.6.3.2-235-distro.tar.gz
oozie.port=20113
oozie.host=127.0.0.1
oozie.sharelib.component=OOZIE,MAPREDUCE_STREAMING,SPARK
#oozie.sharelib.component=OOZIE,HCATALOG,DISTCP,MAPREDUCE_STREAMING,PIG,HIVE,HIVE2,SQOOP,SPARK

# ElasticSearch
elasticsearch.version=6.2.4
elasticsearch.ip=127.0.0.1
elasticsearch.http.port=14433
elasticsearch.tcp.port=14533
elasticsearch.index.name=test_index
elasticsearch.cluster.name=elasticsearch
#elasticsearch.download.url=https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.4.3.zip

# MongoDB
mongo.ip=127.0.0.1
mongo.port=13333
mongo.database.name=test_database
mongo.collection.name=test_collection

# Cassandra
cassandra.ip=127.0.0.1
cassandra.port=13433
cassandra.temp.dir=/tmp/embedded_cassandra

# Neo4j
neo4j.ip=127.0.0.1
neo4j.port=13533
neo4j.temp.dir=/tmp/embedded_neo4j

# KNOX
knox.host=127.0.0.1
knox.port=8888
knox.path=gateway
knox.cluster=mycluster
knox.home.dir=/tmp/embedded_knox
knox.service=namenode,webhdfs,webhbase
#knox.service=namenode,webhdfs,webhbase,oozie

# Alluxio
#alluxio.work.dir=/tmp/alluxio
alluxio.work.dir=hdfs://127.0.0.1:20112/alluxio
alluxio.hostname=127.0.0.1
alluxio.master.port=14001
alluxio.master.web.port=14002
alluxio.proxy.web.port=14100
alluxio.worker.web.port=14003
alluxio.worker.data.port=14004
alluxio.worker.port=14005
alluxio.webapp.directory=conf/alluxio/webapp


# Redis
redis.port=6379
redis.download.url=http://download.redis.io/releases/
redis.version=4.0.0
redis.cleanup.installation=false
redis.temp.dir=/tmp/redis
redis.type=SERVER
#redis.type=CLUSTER
#redis.type=MASTER_SLAVE
#redis.type=SENTINEL
#redis.slave.ports=6380
#redis.sentinel.ports=36479,36480,36481,36482,36483




# Confluent
confluent.schemaregistry.port=8081
confluent.schemaregistry.host=127.0.0.1
confluent.schemaregistry.kafkastore.topic=_schema
confluent.schemaregistry.debug=false

confluent.kafka.log.dirs=/tmp/kafka-logs
confluent.kafka.broker.id=0
confluent.kafka.port=22222
confluent.kafka.host=127.0.0.1

confluent.rest.host=127.0.0.1
confluent.rest.port=8082

confluent.ksql.host=127.0.0.1
confluent.ksql.port=8083



# Docker
docker.imagename=alpine:3.2
docker.exposedports=80
docker.envs=MAGIC_NUMBER:42
docker.labels=MAGIC_NUMBER:42
docker.command=/bin/sh, -c, while true; do echo \"$MAGIC_NUMBER\" | nc -l -p 80; done
docker.fixed.exposedports=21300:80
#docker.classpath.resources.mapping=hadoop-unit-default.properties:/hadoop-unit-default.properties:READ_ONLY

# Docker compose
dockercompose.filename=conf/docker-compose.yml
#dockercompose.exposedports=zoo:2181,resourcemanager:8088
dockercompose.local=false